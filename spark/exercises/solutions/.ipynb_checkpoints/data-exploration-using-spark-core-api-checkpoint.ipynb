{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Firstly, upload the data file in HDFS using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put  ~/training_materials/data/sparkintro.txt /user/cloudera/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will put the file in the home directory of the user in HDFS  \n",
    "-hdfs dfs is a command line client to interact with the HDFS API   \n",
    "-put is used to copy file from local file system to HDFS  \n",
    "-source is the path of the file in the local file system  \n",
    "-destination is the path of the file in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Review the text file ‘sparkintro.txt’ as we will be using this file further, using the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark is a fast and general engine for large-scale data processing.\r",
      "\r\n",
      "Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.\r",
      "\r\n",
      "Apache Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing.\r",
      "\r\n",
      "Write applications quickly in Java, Scala, Python, R.\r",
      "\r\n",
      "Spark offers over 80 high-level operators that make it easy to build parallel apps. \r",
      "\r\n",
      "And you can use it interactively from the Scala, Python and R shells.\r",
      "\r\n",
      "Combine SQL, streaming, and complex analytics.\r",
      "\r\n",
      "Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. \r",
      "\r\n",
      "You can combine these libraries seamlessly in the same application.\r",
      "\r\n",
      "Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.\r",
      "\r\n",
      "You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, or on Apache Mesos. \r",
      "\r\n",
      "Access data in HDFS, Cassandra, HBase, Hive, Tachyon, and any Hadoop data source.\r",
      "\r\n",
      "Python is a programming language that lets you work quickly and integrate systems more effectively.\r",
      "\r\n",
      "Documentation for Python's standard library, along with tutorials and guides, are available online.\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat sparkintro.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-cat is used to display the contents of a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create your first RDD which will load the file in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=sc.textFile(\"sparkintro.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sc.textFile(\"filename\") is used to load a file in memory  \n",
    "Till now, Spark has not yet read the file.  \n",
    "Spark does not do the work until the action is performed on the RDD.  \n",
    "You can validate this by specifying a wrong file name here, it will give error only when you take action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Count number of elements in the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-count() is an action that returns the number of elements in a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Split the RDD based on ‘  ‘ delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_split=data.map(lambda line: line.split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This will split the RDD based on new line as a delimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Print the array of all elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Apache Spark is a fast and general engine for large-scale data processing.'],\n",
       " [u'Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.'],\n",
       " [u'Apache Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing.'],\n",
       " [u'Write applications quickly in Java, Scala, Python, R.'],\n",
       " [u'Spark offers over 80 high-level operators that make it easy to build parallel apps. '],\n",
       " [u'And you can use it interactively from the Scala, Python and R shells.'],\n",
       " [u'Combine SQL, streaming, and complex analytics.'],\n",
       " [u'Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. '],\n",
       " [u'You can combine these libraries seamlessly in the same application.'],\n",
       " [u'Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.'],\n",
       " [u'You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, or on Apache Mesos. '],\n",
       " [u'Access data in HDFS, Cassandra, HBase, Hive, Tachyon, and any Hadoop data source.'],\n",
       " [u'Python is a programming language that lets you work quickly and integrate systems more effectively.'],\n",
       " [u\"Documentation for Python's standard library, along with tutorials and guides, are available online.\"]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-collect() is an action that prints the array of all elements in the text file.  \n",
    "Note: collect operation is not advisable for large datasets, try avoid using it as it may bring down Spark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Print only 3 elements of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Apache Spark is a fast and general engine for large-scale data processing.'],\n",
       " [u'Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.'],\n",
       " [u'Apache Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-take(n) is an action that will print the n elements of the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Filter the elements based upon the elements that contain the word \"Spark\" and then print all the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Apache Spark is a fast and general engine for large-scale data processing.',\n",
       " u'Apache Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing.',\n",
       " u'Spark offers over 80 high-level operators that make it easy to build parallel apps. ',\n",
       " u'Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. ',\n",
       " u'Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.',\n",
       " u'You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, or on Apache Mesos. ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filter=data.filter(lambda words: \"Spark\" in words)\n",
    "data_filter.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will filter only those elements that contain the word \"Spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Save the elements as a textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_filter.saveAsTextFile(\"spark_intro_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-saveAsTextFile(...) is used to store the elements of an RDD in local file system or HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10: You can review the output using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark is a fast and general engine for large-scale data processing.\r\n",
      "Apache Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing.\r\n",
      "Spark offers over 80 high-level operators that make it easy to build parallel apps. \r\n",
      "Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. \r\n",
      "Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.\r\n",
      "You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, or on Apache Mesos. \r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat spark_intro_output/part*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
